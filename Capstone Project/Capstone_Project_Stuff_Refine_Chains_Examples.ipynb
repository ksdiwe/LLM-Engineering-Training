{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "93b70a29-02dc-4131-9268-c59a8f97f2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import StuffDocumentsChain, RefineDocumentsChain\n",
    "from langchain.schema import Document\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efeec15e-b8a1-49b6-bfa2-0e6cedef5ffc",
   "metadata": {},
   "source": [
    "# Reading and Preprocessing Document Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "76244d10-0e4f-4969-b309-97b434e5dcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"Benchmark-GLUE-data-pdf.pdf\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "686a1264-8416-4b95-a28e-fb027c9ffc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages loaded: 20\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of pages loaded: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "2ea15e83-58af-475e-9c00-da06435967c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1\n",
      "Metadata: {'source': 'Benchmark-GLUE-data-pdf.pdf', 'page': 0}\n",
      "Content:  glue: a m ulti -task benchmark and analysis platform for natural language understand - ing alex wang1, amanpreet singh1, julian michael2, felix hill3, omer levy2  samuel r. bowman1 1courant institute of mathematical sciences, new york university 2paul g. allen school of computer science  engineering, university of washington 3deepmind    abstract for natural language understanding nlu technology to be maximally useful, it must be able to process language in a way that is not exclusive to a sing\n",
      "----------------------------------------\n",
      "Document 2\n",
      "Metadata: {'source': 'Benchmark-GLUE-data-pdf.pdf', 'page': 1}\n",
      "Content:  corpus train  test task metrics domain single-sentence tasks cola 8.5k 1k acceptability matthews corr. misc. sst-2 67k 1.8k sentiment acc. movie reviews similarity and paraphrase tasks mrpc 3.7k 1.7k paraphrase acc.f1 news sts-b 7k 1.4k sentence similarity pearsonspearman corr. misc. qqp 364k 391k paraphrase acc.f1 social qa questions inference tasks mnli 393k 20k nli matched acc.mismatched acc. misc. qnli 105k 5.4k qanli acc. wikipedia rte 2.5k 3k nli acc. news, wikipedia wnli 634 146 corefere\n",
      "----------------------------------------\n",
      "Document 3\n",
      "Metadata: {'source': 'Benchmark-GLUE-data-pdf.pdf', 'page': 2}\n",
      "Content:  2005, sst socher et al., 2013, cr hu  liu, 2004, and subj pang  lee, 2004. other tasks are so close to being solved that evaluation on them is relatively uninformative, such as mpqa wiebe et al., 2005 and trec question classication v oorhees et al., 1999. in glue, we attempt to construct a benchmark that is both diverse and difcult. mccann et al. 2018 introduce decanlp, which also scores nlp systems based on their perfor- mance on multiple datasets. their benchmark recasts the ten evaluation ta\n",
      "----------------------------------------\n",
      "Document 4\n",
      "Metadata: {'source': 'Benchmark-GLUE-data-pdf.pdf', 'page': 3}\n",
      "Content:  coarse-grained categories fine-grained categories lexical semantics lexical entailment, morphological negation, factivity, symmetrycollectivity, redundancy, named entities, quantiers predicate-argument structure core arguments, prepositional phrases, ellipsisimplicits, anaphoracoreference activepassive, nominalization, genitivespartitives, datives, relative clauses, coordination scope, intersectivity, restrictivity logic negation, double negation, intervalsnumbers, conjunction, disjunction, con\n",
      "----------------------------------------\n",
      "Document 5\n",
      "Metadata: {'source': 'Benchmark-GLUE-data-pdf.pdf', 'page': 4}\n",
      "Content:  tags sentence 1 sentence 2 fwd bwd lexical entailment lexi- cal semantics, downward monotone logic the timing of the meeting has not been set, according to a starbucks spokesper- son. the timing of the meet- ing has not been consid- ered, according to a star- bucks spokesperson. n e universal quantiers logic our deepest sympathies are with all those affected by this accident. our deepest sympathies are with a victim who was af- fected by this accident. e n quantiers lexical se- mantics, double \n",
      "----------------------------------------\n",
      "Document 6\n",
      "Metadata: {'source': 'Benchmark-GLUE-data-pdf.pdf', 'page': 5}\n",
      "Content:  annotation process we begin with a target set of phenomena, based roughly on those used in the fracas suite cooper et al., 1996. we construct each example by locating a sentence that can be easily made to demonstrate a target phenomenon, and editing it in two ways to produce an appro- priate sentence pair. we make minimal modications so as to maintain high lexical and structural overlap within each sentence pair and limit supercial cues. we then label the inference relation- ships between the s\n",
      "----------------------------------------\n",
      "Document 7\n",
      "Metadata: {'source': 'Benchmark-GLUE-data-pdf.pdf', 'page': 6}\n",
      "Content:  single sentence similarity and paraphrase natural language inference model avg cola sst-2 mrpc qqp sts-b mnli qnli rte wnli single-task training bilstm 63.9 15.7 85.9 69.379.4 81.761.4 66.062.8 70.370.8 75.7 52.8 65.1 elmo 66.4 35.0 90.2 69.080.8 85.765.6 64.060.2 72.973.4 71.7 50.1 65.1 cove 64.0 14.5 88.5 73.4 81.4 83.359.4 67.2 64.1 64.564.8 75.4 53.5 65.1 attn 63.9 15.7 85.9 68.580.3 83.562.9 59.355.8 74.273.8 77.2 51.9 65.1 attn, elmo 66.5 35.0 90.2 68.880.2 86.566.1 55.552.5 76.976.7 76.7\n",
      "----------------------------------------\n",
      "Document 8\n",
      "Metadata: {'source': 'Benchmark-GLUE-data-pdf.pdf', 'page': 7}\n",
      "Content:  coarse-grained fine-grained model all ls pas l k uquant mneg 2neg coref restr down single-task training bilstm 21 25 24 16 16 70 53 4 21 -15 12 elmo 20 20 21 14 17 70 20 42 33 -26 -3 cove 21 19 23 20 18 71 47 -1 33 -15 8 attn 25 24 30 20 14 50 47 21 38 -8 -3 attn, elmo 28 30 35 23 14 85 20 42 33 -26 -3 attn, cove 24 29 29 18 12 77 50 1 18 -1 12 multi-task training bilstm 20 13 24 14 22 71 17 -8 31 -15 8 elmo 21 20 21 19 21 71 60 2 22 0 12 cove 18 15 11 18 27 71 40 7 40 0 8 attn 18 13 24 11 16 7\n",
      "----------------------------------------\n",
      "Document 9\n",
      "Metadata: {'source': 'Benchmark-GLUE-data-pdf.pdf', 'page': 8}\n",
      "Content:  7 a nalysis we analyze the baselines by evaluating each models mnli classier on the diagnostic set to get a better sense of their linguistic capabilities. results are presented in table 5. coarse categories overall performance is low for all models: the highest total score of 28 still denotes poor absolute performance. performance tends to be higher on predicate-argument struc- ture and lower on logic, though numbers are not closely comparable across categories. unlike on the main benchmark, th\n",
      "----------------------------------------\n",
      "Document 10\n",
      "Metadata: {'source': 'Benchmark-GLUE-data-pdf.pdf', 'page': 9}\n",
      "Content:  references dzmitry bahdanau, kyunghyun cho, and yoshua bengio. neural machine translation by jointly learning to align and translate. in proceedings of the international conference on learning rep- resentations, 2015. roy bar haim, ido dagan, bill dolan, lisa ferro, danilo giampiccolo, bernardo magnini, and idan szpektor. the second pascal recognising textual entailment challenge. 2006. luisa bentivogli, ido dagan, hoa trang dang, danilo giampiccolo, and bernardo magnini. the fth pascal recogni\n",
      "----------------------------------------\n",
      "Document 11\n",
      "Metadata: {'source': 'Benchmark-GLUE-data-pdf.pdf', 'page': 10}\n",
      "Content:  jan gorodkin. comparing two k-category assignments by a k-category correlation coefcient.com- put. biol. chem., 285-6:367374, december 2004. issn 1476-9271. suchin gururangan, swabha swayamdipta, omer levy, roy schwartz, samuel r. bowman, and noah a. smith. annotation artifacts in natural language inference data. in proceedings of the north american chapter of the association for computational linguistics: human language technologies, 2018. kazuma hashimoto, caiming xiong, yoshimasa tsuruoka, a\n",
      "----------------------------------------\n",
      "Document 12\n",
      "Metadata: {'source': 'Benchmark-GLUE-data-pdf.pdf', 'page': 11}\n",
      "Content:  jeffrey pennington, richard socher, and christopher manning. glove: global vectors for word representation. in proceedings of the conference on empirical methods in natural language processing, pp. 15321543, 2014. matthew e peters, mark neumann, mohit iyyer, matt gardner, christopher clark, kenton lee, and luke zettlemoyer. deep contextualized word representations. in proceedings of the north amer- ican chapter of the association for computational linguistics: human language technologies , 2018\n",
      "----------------------------------------\n",
      "Document 13\n",
      "Metadata: {'source': 'Benchmark-GLUE-data-pdf.pdf', 'page': 12}\n",
      "Content:  single sentence similarity and paraphrase natural language inference model avg cola sst-2 mrpc qqp sts-b mnli qnli rte wnli single-task training bilstm 66.7 17.6 87.5 77.985.1 85.382.0 71.672.0 66.7 77.0 58.5 56.3 elmo 68.7 44.1 91.5 70.882.3 88.084.3 70.370.5 68.6 71.2 53.4 56.3 cove 66.8 25.1 89.2 76.583.4 86.281.8 70.770.8 62.4 74.4 59.6 54.9 attn 66.9 17.6 87.5 72.882.9 87.783.9 66.666.7 70.0 77.2 58.5 60.6 attn, elmo 67.9 44.1 91.5 71.182.1 87.883.6 57.956.1 72.4 75.2 52.7 56.3 attn, cove \n",
      "----------------------------------------\n",
      "Document 14\n",
      "Metadata: {'source': 'Benchmark-GLUE-data-pdf.pdf', 'page': 13}\n",
      "Content:  b a dditional baseline details b.1 a ttention mechanism we implement our attention mechanism as follows: given two sequences of hidden states u1,u2,...,u m and v1,v2,...,v n , we rst compute matrix h where hij  ui vj. for each ui, we get attention weights i by taking a softmax over the ith row of h, and get the correspond- ing context vector vi   j ijvj by taking the attention-weighted sum of the vj. we pass a second bilstm with max pooling over the sequence u1; v1,... um ; vm  to produce u. we\n",
      "----------------------------------------\n",
      "Document 15\n",
      "Metadata: {'source': 'Benchmark-GLUE-data-pdf.pdf', 'page': 14}\n",
      "Content:  figure 1: the benchmark website leaderboard. an expanded view shows additional details about each submission, including a brief prose description and parameter count. category count  neutral  contradiction  entailment lexical semantics 368 31.0 27.2 41.8 predicate-argument structure 424 37.0 13.7 49.3 logic 364 37.6 26.9 35.4 knowledge 284 26.4 31.7 41.9 table 7: diagnostic dataset statistics by coarse-grained category. note that some examples may be tagged with phenomena belonging to multiple \n",
      "----------------------------------------\n",
      "Document 16\n",
      "Metadata: {'source': 'Benchmark-GLUE-data-pdf.pdf', 'page': 15}\n",
      "Content:  e.1 l exical semantics these phenomena center on aspects of word meaning. lexical entailment entailment can be applied not only on the sentence level, but the word level. for example, we say dog lexically entails animal because anything that is a dog is also an animal, and dog lexically contradicts cat because it is impossible to be both at once. this rela- tionship applies to many types of words nouns, adjectives, verbs, many prepositions, etc. and the relationship between lexical and sententi\n",
      "----------------------------------------\n",
      "Document 17\n",
      "Metadata: {'source': 'Benchmark-GLUE-data-pdf.pdf', 'page': 16}\n",
      "Content:  named entities words often name entities that exist in the world. there are many different kinds of understanding we might wish to understand about these names, including their compositional structure for example, the baltimore police is the same as the police of the city of baltimore or their real-world referents and acronym expansions for example, snl is saturday night live. this category is closely related to world knowledge, but focuses on the semantics of names as lexical items rather than\n",
      "----------------------------------------\n",
      "Document 18\n",
      "Metadata: {'source': 'Benchmark-GLUE-data-pdf.pdf', 'page': 17}\n",
      "Content:  anaphoracoreference coreference refers to when multiple expressions refer to the same entity or event. it is closely related to anaphora, where the meaning of an expression depends on another antecedent expression in context. these two phenomena have signicant overlap; for example, pronouns she, we, it are anaphors that are co-referent with their antecedents. however, they also may occur independently, such as coreference between two denite noun phrases e.g., theresa may and the british prime m\n",
      "----------------------------------------\n",
      "Document 19\n",
      "Metadata: {'source': 'Benchmark-GLUE-data-pdf.pdf', 'page': 18}\n",
      "Content:  conjunction: temperature and snow consistency must be just right entails temperature must be just right. disjunction: life is either a daring adventure or nothing at all does not entail, but is entailed by, life is a daring adventure. conditionals: if both apply, they are essentially impossible does not entail they are essentially impossible. conditionals are more complicated because their use in language does not always mirror their mean- ing in logic. for example, they may be used at a higher\n",
      "----------------------------------------\n",
      "Document 20\n",
      "Metadata: {'source': 'Benchmark-GLUE-data-pdf.pdf', 'page': 19}\n",
      "Content:  e.4 k nowledge strictly speaking, world knowledge and common sense are required on every level of language understanding for disambiguating word senses, syntactic structures, anaphora, and more. so our entire suite and any test of entailment does test these features to some degree. however, in these categories, we gather examples where the entailment rests not only on correct disambiguation of the sentences, but also application of extra knowledge, whether concrete knowledge about world affairs\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to preprocess the text\n",
    "def preprocess_text(text):\n",
    "    # 1. Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # 2. Remove emails\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # 3. Remove special characters (except basic punctuation)\n",
    "    text = re.sub(r'[^a-zA-Z0-9.,;:\\'\"\\s-]', '', text)\n",
    "    \n",
    "    # 4. Remove numbers if unnecessary\n",
    " #   text = re.sub(r'\\b\\d+\\b', '', text)\n",
    "    \n",
    "    # 5. Convert to lowercase for uniformity\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 6. Remove headers/footers if present\n",
    "    text = re.sub(r'published as a conference paper.*?iclr \\d{4}', '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Return cleaned text\n",
    "    return text\n",
    "\n",
    "# Preprocess each document's content\n",
    "preprocessed_documents = [\n",
    "    Document(metadata=doc.metadata, page_content=preprocess_text(doc.page_content))\n",
    "    for doc in documents\n",
    "]\n",
    "\n",
    "# Output the preprocessed documents\n",
    "for idx, doc in enumerate(preprocessed_documents):\n",
    "    print(f\"Document {idx + 1}\")\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "    print(f\"Content: {doc.page_content[:500]}\")  # Print the first 500 characters of content\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "19131afc-279b-4a47-857f-29434cb8c5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1\n",
      "Metadata: {'source': 'Benchmark-GLUE-data-pdf.pdf', 'page': 0}\n",
      "Content: glue: a m ulti -task benchmark and analysis platform for natural language understand - ing alex wang1, amanpreet singh1, julian michael2, felix hill3, omer levy2 samuel r. bowman1 1courant institute o\n",
      "----------------------------------------\n",
      "Chunk 2\n",
      "Metadata: {'source': 'Benchmark-GLUE-data-pdf.pdf', 'page': 0}\n",
      "Content: g. allen school of computer science engineering, university of washington 3deepmind abstract for natural language understanding nlu technology to be maximally useful, it must be able to process langua\n",
      "----------------------------------------\n",
      "Chunk 3\n",
      "Metadata: {'source': 'Benchmark-GLUE-data-pdf.pdf', 'page': 0}\n",
      "Content: genre, or dataset. in pursuit of this objective, we introduce the general language understanding evaluation glue benchmark, a collection of tools for evaluat- ing the performance of models across a di\n",
      "----------------------------------------\n",
      "Chunk 4\n",
      "Metadata: {'source': 'Benchmark-GLUE-data-pdf.pdf', 'page': 0}\n",
      "Content: with limited training data, glue is designed to favor and encour- age models that share general linguistic knowledge across tasks. glue also in- cludes a hand-crafted diagnostic test suite that enable\n",
      "----------------------------------------\n",
      "Chunk 5\n",
      "Metadata: {'source': 'Benchmark-GLUE-data-pdf.pdf', 'page': 0}\n",
      "Content: we evaluate baselines based on current methods for transfer and rep- resentation learning and nd that multi-task training on all tasks performs better than training a separate model per task. however,\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the character splitter\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\" \",  # Use space as the separator\n",
    "    chunk_size=300,  # Define the maximum size of each chunk\n",
    "    chunk_overlap=50  # Overlap between chunks to maintain context\n",
    ")\n",
    "\n",
    "# Apply the splitter to preprocessed documents\n",
    "split_documents = []\n",
    "for doc in preprocessed_documents:\n",
    "    chunks = text_splitter.split_text(doc.page_content)\n",
    "    for chunk in chunks:\n",
    "        # Create new Document objects for each chunk\n",
    "        split_documents.append(Document(metadata=doc.metadata, page_content=chunk))\n",
    "\n",
    "# Output the split documents\n",
    "for idx, doc in enumerate(split_documents[:5]):  # Display only the first 5 chunks\n",
    "    print(f\"Chunk {idx + 1}\")\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "    print(f\"Content: {doc.page_content[:200]}\")  # Print the first 200 characters\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "5de9dada-d4eb-4a67-a229-4071886583ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "287"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(split_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1d8118d-dc59-4c3e-a012-685bcbf218e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key= '<your_api_key>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "42eb7980-4578-4833-94c0-eb016e685abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm   = ChatOpenAI(openai_api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef23fb30-f0a3-4e19-94a5-4545f7866127",
   "metadata": {},
   "source": [
    "# STUFF CHAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc5689e-239a-4b54-b2b0-8230e8adcee1",
   "metadata": {},
   "source": [
    "# StuffChain: Combining Multiple Documents into a Single Summary\n",
    "\n",
    "**StuffChain** is a powerful chain in LangChain designed to process multiple documents by concatenating their content and generating a unified response. This chain allows you to efficiently summarize or process a set of documents as a whole. The approach is particularly useful when you need to analyze or extract insights from several related documents, ensuring a cohesive and comprehensive result.\n",
    "\n",
    "### How It Works:\n",
    "1. **Input**: A list of documents is provided as input, each containing relevant information.\n",
    "2. **Processing**: The content of these documents is combined into a single string (or chunk of text), which is then fed into the language model for summarization or other tasks.\n",
    "3. **Output**: A unified output, typically in the form of a summary or an aggregated result, is returned, offering insights from the entire collection of documents.\n",
    "\n",
    "### Use Cases:\n",
    "- **Document Summarization**: When you need to summarize a set of related documents into one cohesive summary.\n",
    "- **Information Extraction**: To extract key insights from a series of documents for further analysis.\n",
    "- **Data Aggregation**: Combining data from multiple sources into a single, coherent response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "3bd6a9f3-d4f4-4f88-9522-192e2d64e48f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The text introduces the General Language Understanding Evaluation (GLUE) benchmark, which is a collection of tools for evaluating the performance of natural language understanding models across various tasks. GLUE aims to encourage models that share general linguistic knowledge and includes a diagnostic test suite for detailed linguistic analysis. The text finds that multi-task training on all tasks performs better than training a separate model per task, but indicates the need for improved general NLU systems.'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_prompt = PromptTemplate(input_variables= [\"text\"], \n",
    "                                template       = \"Summarize the following text: {text}\")\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, \n",
    "                     prompt=summary_prompt)\n",
    "\n",
    "stuff_chain = StuffDocumentsChain(llm_chain=llm_chain)\n",
    "\n",
    "summary = stuff_chain.invoke(input={\"input_documents\": split_documents[:5]})\n",
    "summary['output_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9824ade4-f89f-43b9-a429-826d0e9261cb",
   "metadata": {},
   "source": [
    "# REFINE CHAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076c1d21-7567-4c8f-9e34-40f0816c18dc",
   "metadata": {},
   "source": [
    "# RefineChain: Iterative Refinement for Enhanced Summarization\n",
    "\n",
    "**RefineChain** is a versatile chain in LangChain designed for **iterative refinement** of document summaries or responses. This approach involves progressively improving the output by taking an initial summary or answer and refining it with additional context or information. RefineChain is particularly useful when you want to enhance the quality of the generated text over multiple iterations, ensuring more accurate, coherent, and comprehensive results.\n",
    "\n",
    "### How It Works:\n",
    "1. **Input**: The chain starts with an initial response or summary, which is typically based on the first set of documents.\n",
    "2. **Refinement**: The initial summary is iteratively refined by incorporating additional documents or new information. Each iteration helps to clarify or expand on the response, improving its quality.\n",
    "3. **Output**: After the refinement process, the final output is a more polished and detailed summary or answer, reflecting the combined insights from the original and additional information.\n",
    "\n",
    "### Use Cases:\n",
    "- **Improving Summaries**: If you start with a rough summary of a set of documents, RefineChain allows you to incrementally refine it, adding more context or information as needed.\n",
    "- **Detailed Responses**: When generating answers to complex questions, you can start with an initial answer and then refine it by introducing more documents or data points.\n",
    "- **Contextual Expansion**: Ideal for scenarios where the initial summary or answer needs further expansion and deeper understanding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "76ee4252-13bc-47ce-93c1-0eed0b837c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_prompt = PromptTemplate(\n",
    "    input_variables= [\"page_content\"], \n",
    "    template       = \"Generate a brief overview based on the following information: {page_content}\"\n",
    ")\n",
    "\n",
    "refine_prompt = PromptTemplate(\n",
    "    input_variables = [\"existing_answer\", \"new_information\"], \n",
    "    template        = \"Refine the following answer by incorporating this new information: {new_information}. Answer: {existing_answer}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ebc0e88d-cbfb-467f-a272-f3c599b96968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an initial LLMChain for the first response\n",
    "initial_chain = LLMChain(llm=llm, prompt=initial_prompt)\n",
    "\n",
    "# Create a refine LLMChain for the refinement process\n",
    "refine_chain = LLMChain(llm=llm, prompt=refine_prompt)\n",
    "\n",
    "# Combine the chains into a RefineDocumentsChain\n",
    "refine_documents_chain = RefineDocumentsChain(\n",
    "    initial_llm_chain    = initial_chain,       # Initial processing chain\n",
    "    refine_llm_chain     = refine_chain,        # Refinement processing chain\n",
    "    initial_response_name= \"initial_response\",  # Name for the initial response variable\n",
    "    verbose              = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "09ffb060-c804-4526-94b1-eb1cb19c27c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Response: Process the first 3 documents\n",
    "initial_response = initial_chain.invoke(input={\"page_content\": \" \".join(doc.page_content for doc in split_documents[:2])})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "8f5e85cc-2f19-4304-b338-0aa2351fdcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Initial Response:\", initial_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "36894ab2-df81-4615-bc0e-8a9bc657f331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Glue is a multi-task benchmark and analysis platform developed by a team of researchers from New York University and the University of Washington. The platform aims to advance natural language understanding (NLU) technology by testing its ability to process language across various tasks, genres, and datasets. By providing a comprehensive evaluation of NLU capabilities, Glue seeks to enhance the overall performance and versatility of NLU technology for real-world applications.'"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "existing_answer = initial_response['text']  # Directly use the 'text' key for the refinement\n",
    "existing_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "0817ec55-f1a9-44f0-9f10-825e6778d34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Information\n",
    "# New Information: Process the next 2 documents (index 3 to 4)\n",
    "new_information = \" \".join(doc.page_content for doc in split_documents[2:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "ac4dc6c0-dd0d-45e4-95d5-2433333dd197",
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_inputs = {\n",
    "    \"existing_answer\": existing_answer,  # Use the initial response output\n",
    "    \"new_information\": new_information\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "b55cc250-1082-4bef-a6d8-7286dae9b6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_refined_response = refine_chain.invoke(input=refine_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "b991b5e7-3a9b-462c-b9cf-fdf48920ed6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Refined Summary: Glue is a multi-task benchmark and analysis platform for natural language understanding developed by a team of researchers from New York University, University of Washington, and DeepMind. The platform aims to improve NLU technology by enabling it to process language in a way that is not limited to a single task, genre, or dataset. By testing models on a variety of tasks, Glue provides a comprehensive evaluation of their performance and helps researchers identify areas for improvement in NLU technology.\n"
     ]
    }
   ],
   "source": [
    "print(\"Final Refined Summary:\", final_refined_response['existing_answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf58520-ec25-405e-a457-1769da971864",
   "metadata": {},
   "source": [
    "## Comparison of Outputs: Refine Chain vs. Stuff Chain\n",
    "\n",
    "### **Output from Refine Chain:**\n",
    "> \"Glue is a multi-task benchmark and analysis platform for natural language understanding developed by a team of researchers from New York University, University of Washington, and DeepMind. The platform aims to improve NLU technology by enabling it to process language in a way that is not limited to a single task, genre, or dataset. By testing models on a variety of tasks, Glue provides a comprehensive evaluation of their performance and helps researchers identify areas for improvement in NLU technology.\"\n",
    "\n",
    "### **Output from Stuff Chain:**\n",
    "> \"The text introduces the General Language Understanding Evaluation (GLUE) benchmark, which is a collection of tools for evaluating the performance of natural language understanding models across various tasks. GLUE aims to encourage models that share general linguistic knowledge and includes a diagnostic test suite for detailed linguistic analysis. The text finds that multi-task training on all tasks performs better than training a separate model per task, but indicates the need for improved general NLU systems.\"\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Differences:**\n",
    "\n",
    "1. **Focus of the Output:**\n",
    "   - **Refine Chain** focuses on describing the **goal of the Glue platform**: to improve NLU technology, evaluate models across tasks, and identify areas for improvement in NLU.\n",
    "   - **Stuff Chain** provides **more detailed information** about the **GLUE benchmark** itself, mentioning its purpose as a collection of tools for evaluation and including a diagnostic test suite for linguistic analysis.\n",
    "\n",
    "2. **Inclusion of the GLUE acronym:**\n",
    "   - **Stuff Chain** explicitly introduces the **GLUE acronym** (General Language Understanding Evaluation) in the first sentence, providing more context about the benchmark.\n",
    "   - **Refine Chain** does not include this acronym and focuses more on the platform's evaluation aspect.\n",
    "\n",
    "3. **Analysis of Multi-Task Training:**\n",
    "   - **Stuff Chain** includes a **mention of multi-task training** and how it outperforms training separate models per task, directly addressing the comparison.\n",
    "   - **Refine Chain** does not mention this aspect explicitly, focusing more on the general goal of the platform and model evaluation.\n",
    "\n",
    "4. **Style and Detail:**\n",
    "   - **Refine Chain** tends to focus on **higher-level goals**, highlighting the comprehensive evaluation and areas for improvement.\n",
    "   - **Stuff Chain** provides a **bit more granular information**, such as mentioning the diagnostic test suite and emphasizing the goal of encouraging models with general linguistic knowledge.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusion:**\n",
    "While both outputs convey similar core information about the **GLUE benchmark** and its purpose, the **Refine Chain** output is more concise and focuses on broader objectives, while the **Stuff Chain** output delves a bit deeper into the specifics of the platform and its diagnostic capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e969a67-ef81-4a4f-b0bb-3472ad9e39e5",
   "metadata": {},
   "source": [
    "| Feature                | Stuff Chain                                             | Refine Chain                                           |\n",
    "|------------------------|--------------------------------------------------------|-------------------------------------------------------|\n",
    "| **What They Are**      | A chain that combines multiple documents into a single input for the model, typically used for generating a summary or response based on all combined content. | A chain designed to improve or refine an initial output by incorporating new information iteratively. |\n",
    "| **Advantages**         | - Simplicity: Easy to implement and understand.<br>- Efficient for generating a response based on a larger context by combining documents. | - Incremental Improvement: Allows for step-by-step enhancement of the output.<br>- Flexibility: Can adapt the existing output with additional context or new information. |\n",
    "| **Disadvantages**      | - Potential Information Loss: Important details may get lost when summarizing multiple documents into one.<br>- Limited Iteration: Does not allow for iterative feedback on individual documents. | - Complexity: More complex to implement and understand due to multiple steps.<br>- Dependency on Initial Output: Relies on the quality of the initial response, which can affect the final output. |\n",
    "| **When to Use**       | - When needing to create a summary or response based on multiple pieces of information at once.<br>- Suitable for scenarios where a quick overview is needed without the need for refinement. | - When there is a need to refine or improve an existing response.<br>- Best used when new information becomes available that should enhance an already generated output. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a170ce32-67e8-40dd-a9f4-c762434e9c13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrapper",
   "language": "python",
   "name": "web_scrape"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
