{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec67b54b-3869-4d61-9aed-cdff57b831d7",
   "metadata": {
    "id": "ec67b54b-3869-4d61-9aed-cdff57b831d7"
   },
   "source": [
    "# **Indexing Techniques**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de84c2f0-b037-4331-88e6-1fa602c62214",
   "metadata": {
    "id": "de84c2f0-b037-4331-88e6-1fa602c62214"
   },
   "source": [
    "All of these new applications rely on vector embeddings, a type of vector data representation that carries within it semantic information that’s critical for the AI to gain understanding and maintain a long-term memory they can draw upon when executing complex tasks.\n",
    "\n",
    "Embeddings are generated by AI models (such as Large Language Models) and have many attributes or features, making their representation challenging to manage. In the context of AI and machine learning, these features represent different dimensions of the data that are essential for understanding patterns, relationships, and underlying structures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d595e7e9-a919-4606-bc18-0fc2401ee024",
   "metadata": {
    "id": "d595e7e9-a919-4606-bc18-0fc2401ee024"
   },
   "source": [
    "With a vector database, we can add knowledge to our AIs, like semantic information retrieval, long-term memory, and more. The diagram below gives us a better understanding of the role of vector databases in this type of application:\n",
    "\n",
    "![](https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fe88ebbacb848b09e477d11eedf4209d10ea4ac0a-1399x537.png&w=1920&q=75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d5fdc4-29ab-451c-946c-fe5df76ad372",
   "metadata": {
    "id": "08d5fdc4-29ab-451c-946c-fe5df76ad372"
   },
   "source": [
    "Here’s a common pipeline for a vector database:\n",
    "![](https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fff9ba425d0c78d696372e0a43ce57851b4f1d4b7-1307x233.png&w=1920&q=75)\n",
    "\n",
    "**Indexing**: The vector database indexes vectors using an algorithm such as PQ, LSH, or HNSW (more on these below). This step maps the vectors to a data structure that will enable faster searching.<br>\n",
    "**Querying**: The vector database compares the indexed query vector to the indexed vectors in the dataset to find the nearest neighbors (applying a similarity metric used by that index)<br>\n",
    "**Post Processing**: In some cases, the vector database retrieves the final nearest neighbors from the dataset and post-processes them to return the final results. This step can include re-ranking the nearest neighbors using a different similarity measure.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1099a876-4a0f-4695-a1a7-5900a8edb99d",
   "metadata": {
    "id": "1099a876-4a0f-4695-a1a7-5900a8edb99d"
   },
   "source": [
    "## **Algorithms**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4662ef22-5d20-4f3d-bd1c-91d04bf7431e",
   "metadata": {
    "id": "4662ef22-5d20-4f3d-bd1c-91d04bf7431e"
   },
   "source": [
    "The following sections will explore several algorithms and their unique approaches to handling vector embeddings. This knowledge will empower you to make informed decisions and appreciate the seamless performance Pinecone delivers as you unlock the full potential of your application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1670dfe6-489e-41a8-b2a8-f39e76d9cb80",
   "metadata": {
    "id": "1670dfe6-489e-41a8-b2a8-f39e76d9cb80"
   },
   "source": [
    "---------\n",
    "### **Flat Index**\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5afb055-927a-45ea-9935-f687d86469f5",
   "metadata": {
    "id": "d5afb055-927a-45ea-9935-f687d86469f5"
   },
   "source": [
    "IndexFlatL2 measures the L2 (or Euclidean) distance between all given points between our query vector, and the vectors loaded into the index. It’s simple, very accurate, but not too fast.\n",
    "\n",
    "<img src=\"https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fea951a4be3acf9d379cc6f922be1468b37b7f9e5-1280x720.png&w=1920&q=75\" width=\"800\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HtgaiKbJ4RML",
   "metadata": {
    "id": "HtgaiKbJ4RML"
   },
   "source": [
    "-------------------\n",
    "#### **Index Efficiency Metrics**\n",
    "\n",
    "<img src=\"https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fca40b2c0017011212a1cf1a98d0fa76f5a96c260-1280x720.png&w=1920&q=75\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1jp3XOpe2ekU",
   "metadata": {
    "id": "1jp3XOpe2ekU"
   },
   "source": [
    "-------\n",
    "### **Inverted File Index**\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_0avLYw32lI1",
   "metadata": {
    "id": "_0avLYw32lI1"
   },
   "source": [
    "The Inverted File Index (IVF) index consists of search scope reduction through clustering. It’s a very popular index as it’s easy to use, with high search-quality and reasonable search-speed.\n",
    "\n",
    "It works on the concept of Voronoi diagrams — also called Dirichlet tessellation (a much cooler name).\n",
    "\n",
    "To understand Voronoi diagrams, we need to imagine our highly-dimensional vectors placed into a 2D space. We then place a few additional points in our 2D space, which will become our ‘cluster’ (Voronoi cells in our case) centroids.\n",
    "\n",
    "We then extend an equal radius out from each of our centroids. At some point, the circumferences of each cell circle will collide with another — creating our cell edges:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OgryoGJS2xMp",
   "metadata": {
    "id": "OgryoGJS2xMp"
   },
   "source": [
    "\n",
    "<img src=\"https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fe68c5241fd2726395449721f5414bc21b038f615-2020x1270.png&w=2048&q=75\" width=\"800\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z1z3dF0m4bO0",
   "metadata": {
    "id": "z1z3dF0m4bO0"
   },
   "source": [
    "-------------------\n",
    "#### **Index Efficiency Metrics**\n",
    "\n",
    "<img src=\"https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fe38ad40fe8c2573c71a5ac4ac92cd58b11833480-1280x720.png&w=1920&q=75\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kBQvfabCDP2Q",
   "metadata": {
    "id": "kBQvfabCDP2Q"
   },
   "source": [
    "------------\n",
    "### **Product Quantization**\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "W7nxEP8NDSm7",
   "metadata": {
    "id": "W7nxEP8NDSm7"
   },
   "source": [
    "Product quantization is the process where each dataset vector is converted into a short memory-efficient representation (called PQ code). Instead of fully keeping all the vectors, their short representations are stored. At the same time, product quantization is a lossy-compression method which results in lower prediction accuracy but in practice, this algorithm works very well.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:828/format:webp/1*98eO9hCC3Wzp8AURuZT-NA.png\" width=\"800\"/>\n",
    "\n",
    "**Qunatized Vectors**: cetroids in respective subspace<br>\n",
    "In product quantization, a cluster ID is often referred to as a reproduction value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b6706f-ab58-4d50-be55-675c267a42e1",
   "metadata": {
    "id": "81b6706f-ab58-4d50-be55-675c267a42e1"
   },
   "source": [
    "----------\n",
    "#### Inference\n",
    "\n",
    "A query vector is divided into subvectors. For each of its subvectors, distances to all the centroids of the corresponding subspace are computed. Ultimately, this information is stored in table d.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:828/format:webp/1*EWRSJOMo13GHAKwsSxosLw.png\" width=\"800\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e3683d-196f-4a78-90e0-701bad1b90b4",
   "metadata": {
    "id": "47e3683d-196f-4a78-90e0-701bad1b90b4"
   },
   "source": [
    "-----------\n",
    "### **Locality Sensitive Hashing**\n",
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca8a85d-22ec-4e07-a330-5cb758d4f19f",
   "metadata": {
    "id": "5ca8a85d-22ec-4e07-a330-5cb758d4f19f"
   },
   "source": [
    "Locality Sensitive Hashing (LSH) is one of the most popular approximate nearest neighbors search (ANNS) methods.\n",
    "\n",
    "At its core, it is a hashing function that allows us to group similar items into the same hash buckets. So, given an impossibly huge dataset — we run all of our items through the hashing function, sorting items into buckets.\n",
    "\n",
    "Unlike most hashing functions, which aim to minimize hashing collisions — LSH algorithms aim to maximize hashing collisions.\n",
    "\n",
    "<img src=\"https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F5a5f928e39a01b106bc5d2e54d1ead84477fc8bd-1280x700.png&w=1920&q=75\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc48839-598a-4d8f-b75d-62d977f1a32c",
   "metadata": {
    "id": "8bc48839-598a-4d8f-b75d-62d977f1a32c"
   },
   "source": [
    "Using the random projection method, we will reduce our highly-dimensional vectors into low-dimensionality binary vectors. Once we have these binary vectors, we can measure the distance between them using Hamming distance.\n",
    "\n",
    "Let’s work through that in a little more detail.\n",
    "\n",
    "<img src=\"https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fc5f4a5d538de6fa256c0828b710fdbf545152289-1400x787.png&w=1920&q=75\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e38ccc5-9e93-4d41-b0b0-575db84f30c8",
   "metadata": {
    "id": "5e38ccc5-9e93-4d41-b0b0-575db84f30c8"
   },
   "source": [
    "-------\n",
    "<img src=\"https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fa7f8e91533957508ab25f70358768ad116ae37b3-1280x720.png&w=1920&q=75\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d9d271-9260-417b-ba77-0a4325bfc56a",
   "metadata": {
    "id": "84d9d271-9260-417b-ba77-0a4325bfc56a"
   },
   "source": [
    "------\n",
    "<img src=\"https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Ff4fbfddbfa7629ea1e7a717777059c6279c4203d-1280x720.png&w=1920&q=75\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c528fd9-8fc6-48cb-8d29-05818f2f2fc8",
   "metadata": {
    "id": "6c528fd9-8fc6-48cb-8d29-05818f2f2fc8"
   },
   "source": [
    "------\n",
    "<img src=\"https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F95721cf0c2c7689aacc6f8fc27e254249b209c19-1280x720.png&w=1920&q=75\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fqxIn5yj3hsH",
   "metadata": {
    "id": "fqxIn5yj3hsH"
   },
   "source": [
    "-------------------\n",
    "#### **Index Efficiency Metrics**\n",
    "\n",
    "<img src=\"https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F02b1b08c1d5eac76f6472454c702cc70edd9d9da-1280x720.png&w=1920&q=75\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x6E1tAQVDvRu",
   "metadata": {
    "id": "x6E1tAQVDvRu"
   },
   "source": [
    "--------\n",
    "### **Hierarchical Navigable Small World (HNSW)**\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OUv45FeVyLBD",
   "metadata": {
    "id": "OUv45FeVyLBD"
   },
   "source": [
    "Hierarchical Navigable Small World (HNSW) graphs are among the top-performing indexes for vector similarity search[1]. HNSW is a hugely popular technology that time and time again produces state-of-the-art performance with super fast search speeds and fantastic recall.\n",
    "\n",
    "We will describe two fundamental techniques that contributed most heavily to HNSW: the probability skip list, and navigable small world graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LAwySxtZyiZn",
   "metadata": {
    "id": "LAwySxtZyiZn"
   },
   "source": [
    "----------\n",
    "#### **Probability Skip List**\n",
    "\n",
    "<img src=\"https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F9065d31e1b2e33ca697a56082f0ece7eff1c2d9b-1920x500.png&w=1920&q=75\" width=\"800\"/>\n",
    "\n",
    "HNSW inherits the same layered format with longer edges in the highest layers (for fast search) and shorter edges in the lower layers (for accurate search).\n",
    "\n",
    "-----------\n",
    "#### **Navigable Small World Graphs**\n",
    "\n",
    "<img src=\"https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F5ca4fca27b2a9bf89b06748b39b7b6238fd4548c-1920x1080.png&w=1920&q=75\" width=\"800\"/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NXxfw8DGzi7C",
   "metadata": {
    "id": "NXxfw8DGzi7C"
   },
   "source": [
    "#### **HNSW**\n",
    "\n",
    "During the search, we enter the top layer, where we find the longest links. These vertices will tend to be higher-degree vertices (with links separated across multiple layers), meaning that we, by default, start in the zoom-in phase described for NSW.\n",
    "\n",
    "We traverse edges in each layer just as we did for NSW, greedily moving to the nearest vertex until we find a local minimum. Unlike NSW, at this point, we shift to the current vertex in a lower layer and begin searching again. We repeat this process until finding the local minimum of our bottom layer — layer 0.\n",
    "\n",
    "\n",
    "<img src=\"https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fe63ca5c638bc3cd61cc1cd2ab33b101d82170426-1920x1080.png&w=1920&q=75\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q6EbvUxx3s-a",
   "metadata": {
    "id": "q6EbvUxx3s-a"
   },
   "source": [
    "-------------------\n",
    "#### **Index Efficiency Metrics**\n",
    "\n",
    "<img src=\"https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F9f9235729775b53415bb17c4ffc6d387a64d4cf9-1280x720.png&w=1920&q=75\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fc89a6",
   "metadata": {},
   "source": [
    "### Referecnces:\n",
    "1. https://www.pinecone.io/learn/series/faiss/vector-indexes/\n",
    "2. https://towardsdatascience.com/similarity-search-product-quantization-b2a1a6397701"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
