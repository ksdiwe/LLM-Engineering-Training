{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder './data' already exists.\n",
      "    CODE            SHORT DESCRIPTION (VALID ICD-10 FY2025)  \\\n",
      "0   A000  Cholera due to Vibrio cholerae 01, biovar chol...   \n",
      "1   A001    Cholera due to Vibrio cholerae 01, biovar eltor   \n",
      "2   A009                               Cholera, unspecified   \n",
      "3  A0100                         Typhoid fever, unspecified   \n",
      "4  A0101                                 Typhoid meningitis   \n",
      "\n",
      "              LONG DESCRIPTION (VALID ICD-10 FY2025) NF EXCL  \n",
      "0  Cholera due to Vibrio cholerae 01, biovar chol...     NaN  \n",
      "1    Cholera due to Vibrio cholerae 01, biovar eltor     NaN  \n",
      "2                               Cholera, unspecified     NaN  \n",
      "3                         Typhoid fever, unspecified     NaN  \n",
      "4                                 Typhoid meningitis     NaN  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "link = 'https://www.cms.gov/files/document/valid-icd-10-list.xlsx'\n",
    "# Download the xlsx file\n",
    "response = requests.get(link)\n",
    "\n",
    "#check if the data folder exists, else create it\n",
    "def check_and_create_folder(folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "        print(f\"Folder '{folder_path}' created.\")\n",
    "    else:\n",
    "        print(f\"Folder '{folder_path}' already exists.\")\n",
    "check_and_create_folder('./data')\n",
    "\n",
    "# Save the downloaded file\n",
    "with open('./data/icd10_codes.xlsx', 'wb') as file:\n",
    "    file.write(response.content)\n",
    "\n",
    "# Load the data into a DataFrame\n",
    "icd_codes = pd.read_excel('./data/icd10_codes.xlsx')\n",
    "print(icd_codes.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speeding Up Embedding Generation:\n",
    "Generating embeddings, especially for large datasets or real-time applications, can be a time-consuming process. Depending on the size of the data and the complexity of the model, embedding generation can be a bottleneck. Fortunately, several strategies can help speed up this process:\n",
    "\n",
    "1. Batch Processing:\n",
    "Batch processing involves grouping multiple inputs (texts, images, or data points) together and passing them through the embedding generation model in parallel. Instead of processing each input sequentially (which can be slow), we process multiple inputs simultaneously.\n",
    "\n",
    "How it works: Modern deep learning frameworks like TensorFlow and PyTorch are optimized for batch operations. By using the model to process batches of inputs (e.g., a batch of 32 or 64 sentences), the process can be parallelized across the GPU, making the computation significantly faster than processing each sentence one by one.\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Parallelization: Using batch processing allows models to take advantage of parallelism, utilizing multiple cores of the CPU or GPU efficiently.\n",
    "Faster Data Loading: Batch processing often results in faster data loading times, as data for multiple instances is preloaded into memory.\n",
    "Lower Latency: With the right batch size, models can reduce the overhead of repeatedly initializing the model for each input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Asynchronous Processing:\n",
    "Asynchronous processing allows for parallel execution of tasks without waiting for each task to finish sequentially. This can significantly speed up the embedding generation process, especially when working with external APIs or I/O-bound operations (e.g., calling cloud services to generate embeddings).\n",
    "\n",
    "How it works: The idea is to make non-blocking calls to the embedding generation process. While one request is being processed, the program can continue processing other requests. When the first request finishes, its result is processed, and the next one is picked up.\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Non-blocking: Asynchronous methods allow for concurrent execution of multiple tasks, improving throughput.\n",
    "Resource Efficiency: The system does not sit idle while waiting for one task to finish but instead performs other tasks, thus making full use of system resources.\n",
    "Example using Pythonâ€™s asyncio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import asyncio\n",
    "# import aiohttp\n",
    "\n",
    "# async def fetch_embedding(text):\n",
    "#     url = \"http://embedding_service/api\"\n",
    "#     async with aiohttp.ClientSession() as session:\n",
    "#         async with session.post(url, json={\"text\": text}) as response:\n",
    "#             return await response.json()\n",
    "\n",
    "# async def generate_embeddings(texts):\n",
    "#     tasks = [fetch_embedding(text) for text in texts]\n",
    "#     embeddings = await asyncio.gather(*tasks)\n",
    "#     return embeddings\n",
    "\n",
    "# # Example usage\n",
    "# texts = [\"Hello, world!\", \"This is an example sentence.\", \"Batch processing speeds up embedding.\"]\n",
    "# embeddings = asyncio.run(generate_embeddings(texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Task pending name='Task-1' coro=<generate_embeddings() running at C:\\Users\\komal.diwe\\AppData\\Local\\Temp\\ipykernel_7564\\3493916112.py:12>>\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "\n",
    "# Fetch embedding function\n",
    "async def fetch_embedding(text):\n",
    "    url = \"http://embedding_service/api\"\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.post(url, json={\"text\": text}) as response:\n",
    "            return await response.json()\n",
    "\n",
    "# Function to generate embeddings\n",
    "async def generate_embeddings(texts):\n",
    "    tasks = [fetch_embedding(text) for text in texts]\n",
    "    embeddings = await asyncio.gather(*tasks)\n",
    "    return embeddings\n",
    "\n",
    "# For running in Jupyter or already running event loops\n",
    "def run_async_code(coro):\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "    except RuntimeError:  # No running event loop\n",
    "        loop = None\n",
    "\n",
    "    if loop and loop.is_running():\n",
    "        # If a loop is already running, use asyncio.ensure_future\n",
    "        import nest_asyncio  # Helps with nested loops\n",
    "        nest_asyncio.apply()\n",
    "        return asyncio.ensure_future(coro)\n",
    "    else:\n",
    "        # Run normally with asyncio.run\n",
    "        return asyncio.run(coro)\n",
    "\n",
    "# Example usage\n",
    "texts = [\"Hello, world!\", \"This is an example sentence.\", \"Batch processing speeds up embedding.\"]\n",
    "embeddings = run_async_code(generate_embeddings(texts))\n",
    "\n",
    "# Print embeddings\n",
    "print(embeddings)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
