{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `openai.ChatCompletion.create()` function is at the core of building intelligent and responsive AI applications. With a range of customizable parameters, it offers developers precise control over how the AI responds to user prompts. \n",
    "\n",
    "In this blog, we‚Äôll break down each parameter and provide insights on when and how to use them effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the `openai.ChatCompletion.create()` function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `openai.ChatCompletion.create()` function enables developers to interact with OpenAI‚Äôs models for generating conversational responses. \n",
    "\n",
    "By configuring parameters, you can customize responses to suit specific tasks such as chatbots, content generation, or structured Q&A systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI \n",
    "from dotenv import load_dotenv \n",
    "import openai\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = 'OpenAI_API_KEY'\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2020 World Series was played at a neutral site due to the COVID-19 pandemic. The games were held at Globe Life Field in Arlington, Texas.\n"
     ]
    }
   ],
   "source": [
    "# basic structure\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who won the World Series in 2020?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we pass the `model` parameter specifying the model (e.g., \"get-3.5-turbo\"). \n",
    "The messages parameter is an array of message objects, each having a \"**role**\" (\"**system**\", \"**user**\", or \"**assistant**\") and \"**content**\" (the actual text of the message)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `system` role: Context and Behavior Setting\n",
    "The `system` role sets the tone and behavior for the assistant throughout the conversation. It serves as the overarching instruction to guide how the AI responds. For instance:\n",
    "\n",
    "* \"You are a helpful assistant.\"\n",
    "* \"You are a coding assistant proficient in Python.\"\n",
    "\n",
    "2. `user` Role: Input from the User\n",
    "The user role captures the prompts or questions from the user. These messages represent the input that drives the conversation. Examples:\n",
    "\n",
    "* \"Tell me a fun fact.\"\n",
    "* \"Explain recursion in simple terms.\"\n",
    "\n",
    "3. `assistant` Role: Responses from the AI\n",
    "The `assistant` role contains the model‚Äôs responses. These messages provide answers, follow-up questions, or additional information based on the user input. Examples:\n",
    "\n",
    "* \"Sure, here‚Äôs a fun fact: Honey never spoils!\"\n",
    "* \"Recursion is when a function calls itself as part of its execution.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How It Works\n",
    "1. `system` Message: Provides high-level context for the assistant‚Äôs behavior.\n",
    "2. `user` Messages: Drive the conversation by presenting queries or instructions.\n",
    "3. `assistant` Messages: Contain model-generated responses to maintain conversational flow.\n",
    "\n",
    "By assigning specific roles, you establish a structured dialogue that the model can follow, enabling meaningful and contextually aware interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature: Controlling Randomness and Diversity of Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `temperature` parameter adjusts the randomness of the AI‚Äôs responses. It accepts values between `0` and `2`, with lower values generating more focused and deterministic outputs.\n",
    "\n",
    "- Low Temperature (e.g., `0.2`): Ideal for factual and analytical tasks.\n",
    "- High Temperature (e.g., `1.0`): Great for creative writing or brainstorming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature=0.2  # Generates precise, less creative responses\n",
    "temperature=1.0  # Generates varied, more imaginative responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How does a penguin build its house?\n",
      "\n",
      "Igloos it together!\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "      {\"role\": \"user\", \"content\": \"Tell me a joke.\"},\n",
    "      {\"role\": \"assistant\", \"content\": \"Why don't scientists trust atoms?\"},\n",
    "      {\"role\": \"user\", \"content\": \"I don't know, why?\"},\n",
    "      {\"role\": \"assistant\", \"content\": \"Because they make up everything!\"}\n",
    "  ],\n",
    "  temperature=0.8\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I hope that joke made you giggle! Would you like to hear another one?\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "      {\"role\": \"user\", \"content\": \"Tell me a joke.\"},\n",
    "      {\"role\": \"assistant\", \"content\": \"Why don't scientists trust atoms?\"},\n",
    "      {\"role\": \"user\", \"content\": \"I don't know, why?\"},\n",
    "      {\"role\": \"assistant\", \"content\": \"Because they make up everything!\"}\n",
    "  ],\n",
    "  temperature=0.9\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Values between 0.2 and 0.8 can be effective. Lower values, like 0.2, result in more focused and predictable responses, whereas higher values, such as 0.8, introduce greater randomness.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top P (Nucleus Sampling): Controlling Response Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `top_p` parameter, also known as _nucleus sampling_, is a powerful method for balancing creativity and control during text generation. \n",
    "It dynamically narrows down the set of candidate tokens to include only those with the highest cumulative probability, ensuring more coherent and meaningful outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How Top-P Sampling Works\n",
    "1. **Token Subset Selection**: At each generation step, the model considers only the smallest subset of tokens whose cumulative probability meets or exceeds a specified threshold ùëù.\n",
    "2. **Random Sampling**: Once the subset is identified, the next token is randomly sampled from this group.\n",
    "3. **Controlled Randomness**: Low-probability tokens outside the subset are ignored, keeping the output focused and coherent.\n",
    "\n",
    "#### Key Insights\n",
    "- Threshold (top_p):\n",
    "    * `top_p = 1.0`: Considers all tokens, effectively no restriction.\n",
    "    * `top_p < 1.0`: Limits token selection to high-probability candidates, enhancing coherence while retaining creativity.\n",
    "- Result:\n",
    "    * More creative and varied output, avoiding deterministic or repetitive responses.\n",
    "    * Ensures randomness remains under control, avoiding irrelevant or nonsensical outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-P vs. Temperature\n",
    "\n",
    "Both `top_p` and `temperature` are used to control randomness, but they work differently:\n",
    "\n",
    "- `Temperature`: Adjusts the entire probability distribution by scaling token probabilities.\n",
    "- `Top-P`: Focuses on a dynamically determined subset of the highest-probability tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, in a galaxy far, far away, there was a curious cat named Whiskers. Whiskers was no ordinary cat - he had always dreamt of exploring the vast unknown of space. One day, he stowed away on a spaceship and found himself hurtling through the stars.\n",
      "\n",
      "As Whiskers floated through the void, he marveled at the beauty of the universe around him. He watched as planets and stars passed by, feeling a sense of wonder and awe. He even caught glimpses of alien creatures and strange, new worlds.\n",
      "\n",
      "But as Whiskers ventured further into space, he began to feel lonely. He missed the comfort of his home and the familiar sights and sounds of Earth. He longed for a warm lap to curl up in and a bowl of milk to drink.\n",
      "\n",
      "Despite his homesickness, Whiskers knew that he had embarked on an incredible adventure. He was determined to make the most of his time in space and embrace the unknown with courage and curiosity.\n",
      "\n",
      "And so, with a meow of determination, Whiskers continued on his journey through the cosmos, eager to discover what wonders lay ahead. And as he floated through the vast expanse of space, he knew that he was a truly adventurous cat, destined for greatness among the stars.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Write a short story about a cat in space.\"}\n",
    "    ],\n",
    "    temperature=0.8,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Takeaways\n",
    "- How it works: Selects tokens from the smallest subset whose cumulative probability meets or exceeds ùëù, then randomly samples from this set.\n",
    "- Result: Balanced creativity and coherence, avoiding irrelevant outputs.\n",
    "- Use Case: Story generation, creative writing, dialogue systems, and brainstorming tasks.\n",
    "\n",
    "By carefully tuning `top_p`, you can generate outputs that are imaginative, engaging, and contextually appropriate while avoiding excessive randomness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Tokens: Limiting the Response Length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The max_tokens parameter defines the maximum number of tokens (words, punctuation, etc.) the AI can use in a single response. This allows you to control the length of the output.\n",
    "\n",
    "- Short Outputs (e.g., 50): Useful for concise responses.\n",
    "- Long Outputs (e.g., 500): Suitable for detailed explanations or stories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens=50  # Short summary\n",
    "max_tokens=500  # In-depth explanation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¬°Claro! Aqu√≠ tienes la traducci√≥n: 'Buenos d√≠as, mi amigo.'\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "      {\"role\": \"user\", \"content\": \"Translate the following English text to Spanish: 'Good morning, my friend.'\"},\n",
    "      {\"role\": \"assistant\", \"content\": \"Of course, here is the translation: 'Buenos d√≠as, mi amigo.'\"}\n",
    "  ],\n",
    "  max_tokens=20\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n: Generating Multiple Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `n` parameter specifies how many responses the model should generate. This is useful for brainstorming, comparing outputs, or selecting the best response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1: 1. Create a photography series that explores a specific theme or concept, such as reflections or shadows.\n",
      "2. Design and build a piece of interactive art that responds to viewer interaction, such as a light installation that changes colors based on sound.\n",
      "3. Write and produce a short film that explores a social issue or personal experience in a unique and thought-provoking way.\n",
      "4. Create a multimedia exhibition that combines visual art, music, and spoken word to tell a cohesive story or convey a specific message.\n",
      "5. Organize a community art project that involves collaboration with local residents to create a large-scale mural or sculpture in a public space.\n",
      "6. Design and develop a mobile app that addresses a specific need or problem in your community, such as a tool to promote sustainability or mental health awareness.\n",
      "7. Curate a virtual reality experience that immerses viewers in a different world or perspective, such as a recreation of a historical event or a simulation of life on another planet.\n",
      "8. Write and illustrate a graphic novel or comic book that tells a unique and engaging story with compelling visuals.\n",
      "9. Create a fashion collection that incorporates sustainable materials and ethical production practices, highlighting the importance of conscious consumption.\n",
      "10. Start a podcast or video series that features interviews with artists, creatives, and innovators from diverse backgrounds to showcase their work and share their stories.\n",
      "Response 2: 1. Create a multimedia art installation using a combination of music, video, and interactive elements.\n",
      "2. Design and build a sustainable, green roof garden on a vacant urban space.\n",
      "3. Write and produce a short film or web series exploring a specific social issue or personal narrative.\n",
      "4. Organize a community mural project where local artists collaborate to create a large-scale outdoor artwork.\n",
      "5. Develop a virtual reality experience that transports users to a different time or place in history.\n",
      "6. Create a pop-up restaurant or food truck concept that features innovative dishes and culinary techniques.\n",
      "7. Launch a community storytelling project that collects and shares personal stories from diverse individuals in your area.\n",
      "8. Design and construct a public parklet or outdoor seating area to enhance public spaces in your neighborhood.\n",
      "9. Produce a podcast series featuring interviews with local entrepreneurs, artists, and changemakers.\n",
      "10. Start a community garden or urban farming project to promote sustainable food production and education in your area.\n",
      "Response 3: 1. Upcycling old furniture or clothing items to give them a new lease on life\n",
      "2. Creating your own recipe book with unique dishes and flavor combinations\n",
      "3. Designing and painting a mural in a public space or your own home\n",
      "4. Starting a community garden or urban farming project\n",
      "5. Writing and illustrating a children's book\n",
      "6. Building a DIY terrarium or indoor garden\n",
      "7. Organizing a neighborhood cleanup or beautification project\n",
      "8. Hosting a craft workshop or DIY tutorial series for beginners\n",
      "9. Collaborating with local artists to create a public art installation\n",
      "10. Designing and sewing your own clothing line or accessories.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What are some creative project ideas?\"}],\n",
    "    n=3  # Generate 3 different responses\n",
    ")\n",
    "for i, choice in enumerate(response.choices):\n",
    "    print(f\"Response {i + 1}: {choice.message.content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stop: Customizing Stop Conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `stop` parameter lets you define sequences that will terminate the AI‚Äôs response. This is particularly useful for structured outputs like Q&A, lists, or code snippets.\n",
    "\n",
    "_Specifying a list of stop words can help restrict the model from using certain words in its responses. Choose stop words that are relevant to your specific application._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop=[\"\\n\"]  # Stops the response at a newline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cat sat on the mat \n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "      {\"role\": \"user\", \"content\": \"Summarize this text: The cat sat on the mat and looked at the sun.\"}\n",
    "  ],\n",
    "  stop=\"and\"\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Penalty: Controlling Repetitive Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `frequency_penalty` parameter reduces the likelihood of the model repeating the same tokens. It‚Äôs a value between -2.0 and 2.0.\n",
    "\n",
    "- Higher Values (e.g., 0.5): Decrease repetition.\n",
    "- Lower Values (e.g., -0.5): Allow more repetition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_penalty=0.5  # Reduces redundant phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Raising this value (e.g., 0.6) helps the model reduce repetition of words or phrases, resulting in more diverse responses._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is a high-level, interpreted programming language known for its simplicity and readability. It was created by Guido van Rossum and first released in 1991. Python has a large standard library that provides useful modules and packages for a wide range of tasks, making it popular among developers for its versatility.\n",
      "\n",
      "Python is often used for web development, data analysis, artificial intelligence, machine learning, scientific computing, and automation tasks. Its syntax is clean and easy to learn, making it a great choice for beginners as well as experienced programmers.\n",
      "\n",
      "One of the key features of Python is its dynamic typing system and strong support for object-oriented programming (OOP), functional programming, and procedural programming paradigms. It also has an extensive ecosystem of third-party libraries and frameworks that further enhance its capabilities. \n",
      "\n",
      "Overall, Python's popularity continues to grow due to its ease of use, community support, and wide range of applications in different fields.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "      {\"role\": \"user\", \"content\": \"What are some popular programming languages in 2024?\"},\n",
    "      {\"role\": \"assistant\", \"content\": \"Some popular programming languages in 2024 include Python, JavaScript, and Go.\"},\n",
    "      {\"role\": \"user\", \"content\": \"Tell me more about Python.\"}\n",
    "  ],\n",
    "  frequency_penalty=0.6\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## log of prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logprobs Attribute Summary\n",
    "\n",
    "| Logprobs | Description                                                                                              | Example                           |\n",
    "|----------|----------------------------------------------------------------------------------------------------------|-----------------------------------|\n",
    "| None     | Indicates that there are no associated log probabilities provided for the completion.                   | \"logprobs\": None                  |\n",
    "| {}       | Represents an empty dictionary, meaning no log probabilities were calculated for this response.         | \"logprobs\": {}                    |\n",
    "| {...}    | Contains a dictionary of log probabilities for each token in the generated text, if applicable.         | \"logprobs\": {\"tokens\": [...], \"token_logprobs\": [...], \"top_logprobs\": [...], \"text_offset\": [...] } |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use Cases for Logprobs Attribute with Example Prompts\n",
    "\n",
    "| Use Case                            | Prompt                                      | Output                                | Logprobs                                                                                                                                                                       | Interpretation                                                                                              |\n",
    "|-------------------------------------|---------------------------------------------|---------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------|\n",
    "| Analyzing Model Confidence           | \"What is the capital of France?\"           | \"Paris\"                               | {\"tokens\": [\"Paris\"], \"token_logprobs\": [-0.1]}                                                                                                                             | A log probability of -0.1 indicates high confidence in \"Paris\" as the correct answer.                      |\n",
    "| Understanding Alternatives           | \"The sky is usually...\"                    | \"blue\"                                | {\"top_logprobs\": [{\"blue\": -0.5}, {\"gray\": -1.2}, {\"green\": -1.5}]}                                                                                                         | The model strongly preferred \"blue,\" with \"gray\" and \"green\" being less likely alternatives.                |\n",
    "| Improving Sampling Strategies        | \"A great place to relax is...\"             | \"the beach\"                           | {\"tokens\": [\"the\", \"beach\"], \"token_logprobs\": [-0.3, -1.0], \"top_logprobs\": [{\"the beach\": -0.3}, {\"a park\": -1.5}, {\"a forest\": -1.8}]}                                 | The model's choice of \"the beach\" indicates a relatively high confidence, suggesting effective sampling.     |\n",
    "| Evaluating Output Quality            | \"What is 2 + 2?\"                           | \"4\"                                   | {\"tokens\": [\"4\"], \"token_logprobs\": [-0.2]}                                                                                                                                 | A low log probability for \"4\" suggests that the model is likely providing a correct and reliable answer.      |\n",
    "| Training and Fine-tuning Insights   | \"The cat chased the...\"                    | \"mouse\"                               | {\"tokens\": [\"mouse\"], \"token_logprobs\": [-2.0]}                                                                                                                             | A low log probability indicates the model struggled to associate \"cat\" with \"mouse,\" signaling a training gap.|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant with master level in General KNowledge.\"},\n",
    "        {\"role\": \"user\",   \"content\": '''What is the capital of Punjab state in India? Only return the name'''},\n",
    "    ],\n",
    "    logprobs = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': [{'token': 'Ch',\n",
       "   'bytes': [67, 104],\n",
       "   'logprob': -1.735894e-05,\n",
       "   'top_logprobs': []},\n",
       "  {'token': 'and',\n",
       "   'bytes': [97, 110, 100],\n",
       "   'logprob': -4.3202e-07,\n",
       "   'top_logprobs': []},\n",
       "  {'token': 'igarh',\n",
       "   'bytes': [105, 103, 97, 114, 104],\n",
       "   'logprob': -4.9617593e-06,\n",
       "   'top_logprobs': []}],\n",
       " 'refusal': None}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view logprobs\n",
    "response.to_dict()['choices'][0]['logprobs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why use log of probs\n",
    "\n",
    "- `logprobs` stands for logarithm of probabilities. It provides the log-probability (logarithmic scale) of each token generated by the model.\n",
    "- These values represent how `likely` the model thinks a particular token should come next, with lower values indicating more probable tokens.\n",
    "- `Log-probabilities` are often preferred because they are more `numerically stable` than raw probabilities, especially when dealing with `very small probability values`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cumulative log-probability for 'Kolkata' is: -2.27527193e-05\n",
      "The probability (confidence) for 'Kolkata' is: 0.9999772475395412\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_final_logprob_and_prob(data):\n",
    "    # Extract log-probabilities from the input data\n",
    "    logprobs_content = data['logprobs']['content']\n",
    "    \n",
    "    # Initialize cumulative log-probability\n",
    "    cumulative_logprob = 0\n",
    "    \n",
    "    # Iterate over each token and sum the log-probabilities\n",
    "    for token_info in logprobs_content:\n",
    "        cumulative_logprob += token_info['logprob']\n",
    "    \n",
    "    # Calculate the final probability (confidence)\n",
    "    final_prob = np.exp(cumulative_logprob)\n",
    "    \n",
    "    return cumulative_logprob, final_prob\n",
    "\n",
    "# Calculate and print the final log-probability and probability\n",
    "final_logprob, final_prob = calculate_final_logprob_and_prob(response.to_dict()['choices'][0])\n",
    "print(f\"The cumulative log-probability for 'Kolkata' is: {final_logprob}\")\n",
    "print(f\"The probability (confidence) for 'Kolkata' is: {final_prob}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logit_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logit bias in OpenAI's GPT models is a powerful tool for influencing the likelihood of specific tokens in the generated output. \n",
    "\n",
    "It allows fine-grained control over the model's behavior by adjusting the probability of specific token outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No\n"
     ]
    }
   ],
   "source": [
    "logit_bias = {\n",
    "    9642: -15,   # Token ID for \"yes\"\n",
    "    2822:  10    # Token ID for \"no\"\n",
    "}\n",
    "prompt = \"Should I invest in this stock? Answer with only Yes or No\"\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "            model      = \"gpt-3.5-turbo\",  # You can use other engines like gpt-3.5-turbo\n",
    "            messages   = [\n",
    "                {\"role\": \"assistant\", \"content\": 'You are a helpful assistant'},\n",
    "                {\"role\": \"user\", \"content\": f'{prompt}'}\n",
    "            ],\n",
    "            max_tokens = 3, \n",
    "            logit_bias = logit_bias\n",
    "        )\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
